{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794efa91",
   "metadata": {},
   "source": [
    "## 1. Connect to Azure OpenAI LLM\n",
    "Initialize and connect to the Azure OpenAI service using credentials from environment variables to enable chat completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7af7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to llm\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(   \n",
    "  model=\"gpt-4o\",#  Replace with your actual deployment name from Azure Portal\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"This is a test.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61699508",
   "metadata": {},
   "source": [
    "## 2. Install Azure Search Documents Library\n",
    "Install the required `azure-search-documents` package to interact with Azure AI Search service.\n",
    "- pip install azure-search-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7005acdd",
   "metadata": {},
   "source": [
    "## 3. Connect to Azure AI Search Service\n",
    "Initialize the SearchClient with endpoint, index name, and API key from environment variables to enable queries against the Azure Search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea22752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "service_endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX_NAME\"]\n",
    "key = os.environ[\"AZURE_SEARCH_API_KEY\"]\n",
    "\n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b98dd",
   "metadata": {},
   "source": [
    "## 4. Search and Display Results\n",
    "Execute a search query against the Azure Search index and display all results with their fields and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb8768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import QueryType\n",
    "\n",
    "# Semantic search with reranking\n",
    "results = search_client.search(\n",
    "    search_text=\"ROBERT AUDI\",\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name=\"default\",  # or your configured semantic config name\n",
    "    top=5  # number of results to return\n",
    ")\n",
    "\n",
    "print(\"Semantic Search Results...\")\n",
    "result_list = list(results)\n",
    "print(f\"Number of results: {len(result_list)}\")\n",
    "\n",
    "if result_list:\n",
    "    print(\"\\nFirst result:\")\n",
    "    print(result_list[0])\n",
    "    \n",
    "    for i, result in enumerate(result_list):\n",
    "        print(f\"\\n--- Result {i+1} ---\")\n",
    "        for key, value in result.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"No results found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af957d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dd9a841",
   "metadata": {},
   "source": [
    "## 8. Compare Normal Search vs Semantic Search\n",
    "\n",
    "### What You'll See\n",
    "\n",
    "This cell performs **both search types** on the same query and compares the results:\n",
    "\n",
    "- **ðŸ”µ Normal Keyword Search**: Matches exact words in your documents\n",
    "- **ðŸŸ¢ Semantic Search**: Understands meaning and context, ranks by relevance\n",
    "\n",
    "### Which One is Better?\n",
    "\n",
    "It depends on your use case:\n",
    "- **Normal Search**: Fast, good for exact matches (\"John Smith\")\n",
    "- **Semantic Search**: Smarter, better at understanding intent (\"Who is the main character?\")\n",
    "\n",
    "**For RAG applications**: Semantic Search usually gives better results because it understands meaning, not just keywords.\n",
    "\n",
    "### Try It!\n",
    "\n",
    "Enter a search query below and compare the results from both search methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import QueryType\n",
    "\n",
    "# Define which fields you want to display\n",
    "# those fields and data just for test that's why the ain't that good \n",
    "IMPORTANT_FIELDS = [\n",
    "    'metadata_storage_name',      # File name\n",
    "    'content',                     # Main content\n",
    "    'keyphrases',                  # Key phrases\n",
    "    'people',                       # People mentioned\n",
    "]\n",
    "\n",
    "user_query = input(\"What do you want to search for? = \")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"QUERY: {user_query}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. NORMAL (KEYWORD) SEARCH\n",
    "# ============================================================================\n",
    "print(\"ðŸ”µ NORMAL KEYWORD SEARCH\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "normal_results = search_client.search(search_text=user_query, top=5)\n",
    "normal_list = list(normal_results)\n",
    "\n",
    "print(f\"Results found: {len(normal_list)}\\n\")\n",
    "\n",
    "if normal_list:\n",
    "    for i, result in enumerate(normal_list, 1):\n",
    "        print(f\"  Result {i}:\")\n",
    "        for key, value in result.items():\n",
    "            # Only show important fields\n",
    "            if key in IMPORTANT_FIELDS:\n",
    "                # Truncate long values for readability\n",
    "                if isinstance(value, str):\n",
    "                    value_str = value[:150] + \"...\" if len(value) > 150 else value\n",
    "                else:\n",
    "                    value_str = str(value)[:150] + \"...\" if len(str(value)) > 150 else str(value)\n",
    "                print(f\"    {key}: {value_str}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"  No results found.\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. SEMANTIC SEARCH\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŸ¢ SEMANTIC SEARCH (AI-Powered Ranking)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "semantic_results = search_client.search(\n",
    "    search_text=user_query,\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name=\"default\",\n",
    "    top=5\n",
    ")\n",
    "\n",
    "semantic_list = list(semantic_results)\n",
    "\n",
    "print(f\"Results found: {len(semantic_list)}\\n\")\n",
    "\n",
    "if semantic_list:\n",
    "    for i, result in enumerate(semantic_list, 1):\n",
    "        print(f\"  Result {i}:\")\n",
    "        for key, value in result.items():\n",
    "            # Only show important fields\n",
    "            if key in IMPORTANT_FIELDS:\n",
    "                # Truncate long values for readability\n",
    "                if isinstance(value, str):\n",
    "                    value_str = value[:150] + \"...\" if len(value) > 150 else value\n",
    "                else:\n",
    "                    value_str = str(value)[:150] + \"...\" if len(str(value)) > 150 else str(value)\n",
    "                print(f\"    {key}: {value_str}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"  No results found.\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š COMPARISON\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Normal Search Results:    {len(normal_list)} documents\")\n",
    "print(f\"Semantic Search Results:  {len(semantic_list)} documents\")\n",
    "print(\"\\nðŸ’¡ Note:\")\n",
    "print(\"  - Normal Search: Matches keywords in documents\")\n",
    "print(\"  - Semantic Search: Understands meaning and context better\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c564ad",
   "metadata": {},
   "source": [
    "# single turn-chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87aac96",
   "metadata": {},
   "source": [
    "## Query Rewriting for Better Search Results\n",
    "\n",
    "Before searching, we optimize the user's natural language question into a search-optimized query. This helps Azure AI Search find more relevant philosophy documents by:\n",
    "- Extracting key philosophical terms and concepts\n",
    "- Removing conversational filler words\n",
    "- Expanding with related philosophical terminology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message for query rewriting - optimized for philosophy searches\n",
    "QUERY_REWRITE_SYSTEM_MESSAGE = \"\"\"You are a query optimization assistant for a philosophy knowledge base (Cambridge Dictionary of Philosophy).\n",
    "\n",
    "Your task: Transform the user's natural language question into an optimized search query for Azure AI Search.\n",
    "\n",
    "Rules:\n",
    "1. Extract key philosophical terms, concepts, and philosopher names\n",
    "2. Remove conversational words (what, how, can you, please, etc.)\n",
    "3. Include relevant philosophical synonyms or related terms\n",
    "4. Keep it concise (3-10 words maximum)\n",
    "5. Return ONLY the optimized query - no explanations, no quotes, no extra text\n",
    "\n",
    "Examples:\n",
    "- User: \"What did Kant think about morality?\" â†’ \"Kant ethics moral philosophy categorical imperative\"\n",
    "- User: \"Can you explain what existentialism means?\" â†’ \"existentialism Sartre Heidegger existence meaning\"\n",
    "- User: \"Who is Plato and what are his main ideas?\" â†’ \"Plato forms idealism Republic Socrates\"\n",
    "- User: \"What's the difference between ethics and morality?\" â†’ \"ethics morality moral philosophy normative\"\n",
    "- User: \"Tell me about free will\" â†’ \"free will determinism libertarianism compatibilism\"\n",
    "\n",
    "Remember: Output ONLY the optimized search query, nothing else.\"\"\"\n",
    "\n",
    "\n",
    "def rewrite_query(user_message: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes a user's natural language question and returns an optimized search query.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": QUERY_REWRITE_SYSTEM_MESSAGE},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_tokens=50,  # Keep responses short\n",
    "        temperature=0.3  # Lower temperature for more consistent outputs\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a10826",
   "metadata": {},
   "source": [
    "# test the query llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the query rewriter\n",
    "test_questions = [\n",
    "    \"What is the meaning of life according to philosophers?\",\n",
    "    \"Can you explain Nietzsche's ideas?\",\n",
    "    \"What's epistemology about?\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ”„ Query Rewriting Examples:\\n\")\n",
    "for q in test_questions:\n",
    "    optimized = rewrite_query(q)\n",
    "    print(f\"Original:  {q}\")\n",
    "    print(f\"Optimized: {optimized}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37082ff9",
   "metadata": {},
   "source": [
    "# Full RAG pipeline with query rewriting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71571408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_message = input(\"Ask a philosophy question: \")\n",
    "\n",
    "# Step 1: Rewrite the query for better search\n",
    "print(f\"\\nðŸ“ Original question: {user_message}\")\n",
    "optimized_query = rewrite_query(user_message)\n",
    "print(f\"ðŸ”„ Optimized query: {optimized_query}\")\n",
    "\n",
    "# Step 2: Search with the optimized query\n",
    "semantic_results = search_client.search(\n",
    "    search_text=optimized_query,  # Use optimized query instead of raw user input\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name=\"default\",\n",
    "    select=[ \"keyphrases\"],  # Exclude 'content'\n",
    "    top=2\n",
    ")\n",
    "semantic_list = list(semantic_results)\n",
    "print(f\" ðŸ“š Found {len(semantic_list)} relevant documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4b1a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(results):## turn the search to markdown \n",
    "    if not results:\n",
    "        return \"No relevant documents found.\"\n",
    "    blocks = []\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        blocks.append(\n",
    "            f\"### Document {i}\\n\"\n",
    "            f\"- **filename:** {doc.get('metadata_storage_name', 'N/A')}\\n\"\n",
    "            f\"- **people:** {', '.join(doc.get('people', [])) if doc.get('people') else 'N/A'}\\n\"\n",
    "            f\"- **keyphrases:** {', '.join(doc.get('keyphrases', [])) if doc.get('keyphrases') else 'N/A'}\\n\\n\"\n",
    "            f\"**content:**\\n{doc.get('content', 'N/A')}\\n\"\n",
    "        )\n",
    "    return \"\\n---\\n\".join(blocks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e776d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust max_content_length based on your needs:\n",
    "# - 2000 chars = ~500 tokens (very concise)\n",
    "# - 4000 chars = ~1000 tokens (good balance)\n",
    "# - 8000 chars = ~2000 tokens (more context)\n",
    "\n",
    "context_md = to_markdown(semantic_list)\n",
    "print(f\"Context length: {len(context_md)} characters (~{len(context_md)//4} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9880fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message for the RAG response generation\n",
    "RAG_SYSTEM_MESSAGE = \"\"\"You are a knowledgeable philosophy assistant powered by the Cambridge Dictionary of Philosophy.\n",
    "\n",
    "Your role: Synthesize search results from the knowledge base to provide accurate, helpful answers to user questions.\n",
    "\n",
    "Guidelines:\n",
    "1. **Use ONLY the provided context** - Do not make up information or use external knowledge\n",
    "2. **Be accurate** - Quote or paraphrase directly from the source documents when possible\n",
    "3. **Be concise** - Provide clear, focused answers without unnecessary filler\n",
    "4. **Cite sources** - Reference document names or philosophers mentioned in the context\n",
    "5. **Acknowledge limitations** - If the context doesn't contain enough information, say so honestly\n",
    "6. **Explain philosophical concepts** - Make complex ideas accessible without oversimplifying\n",
    "7. **Stay neutral** - Present philosophical views objectively without personal bias\n",
    "\n",
    "Response format:\n",
    "- Start with a direct answer to the question\n",
    "- Provide supporting details from the context\n",
    "- Mention relevant philosophers or works if available\n",
    "- Keep responses well-structured and readable\n",
    "\n",
    "If the context is insufficient or irrelevant to the question, respond with:\n",
    "\"I couldn't find enough information in the knowledge base to answer this question accurately.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1db914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send to LLM with proper system message and structured prompt\n",
    "response = client.chat.completions.create(   \n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": RAG_SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": f\"**Context from knowledge base:**\\n{context_md}\\n\\n**User Question:** {user_message}\"}\n",
    "    ],\n",
    "    temperature=0.7,  # Balanced creativity and accuracy\n",
    "    max_tokens=500    # Reasonable response length\n",
    ")\n",
    "\n",
    "print(\"ðŸ¤– Answer:\\n\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0fc39",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c27b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ccab130",
   "metadata": {},
   "source": [
    "# Multi-Turn RAG Chat\n",
    "\n",
    "Now let's implement a **multi-turn conversation** with memory. This allows the assistant to:\n",
    "- Remember previous questions and answers\n",
    "- Handle follow-up questions like \"Tell me more\" or \"What about his other ideas?\"\n",
    "- Maintain context across the conversation\n",
    "\n",
    "## How it works:\n",
    "1. We keep a `conversation_history` list that stores all messages\n",
    "2. Each new question triggers a search â†’ retrieval â†’ response cycle\n",
    "3. The full conversation history is sent to the LLM for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077fd739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn RAG system message (enhanced for conversation context)\n",
    "MULTI_TURN_SYSTEM_MESSAGE = \"\"\"You are a knowledgeable philosophy assistant powered by the Cambridge Dictionary of Philosophy.\n",
    "\n",
    "Your role: Engage in multi-turn conversations about philosophy, using search results to provide accurate answers.\n",
    "\n",
    "Guidelines:\n",
    "1. **Use the provided context** - Base your answers on the search results provided\n",
    "2. **Remember conversation history** - Reference previous exchanges when relevant\n",
    "3. **Handle follow-ups naturally** - Understand questions like \"tell me more\", \"what else?\", \"and his views on X?\"\n",
    "4. **Be accurate and cite sources** - Reference philosophers and works from the context\n",
    "5. **Acknowledge limitations** - If context is insufficient, say so honestly\n",
    "6. **Stay coherent** - Maintain consistency with your previous answers\n",
    "\n",
    "If asked a follow-up without new context, use your previous answers and conversation history.\n",
    "If the context is insufficient, respond: \"I couldn't find enough information to answer this accurately.\"\n",
    "\"\"\"\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "def multi_turn_rag(user_message: str, conversation_history: list) -> str:\n",
    "    \"\"\"\n",
    "    Process a user message in a multi-turn RAG conversation.\n",
    "    \n",
    "    Args:\n",
    "        user_message: The user's current question\n",
    "        conversation_history: List of previous messages [{\"role\": \"user/assistant\", \"content\": \"...\"}]\n",
    "    \n",
    "    Returns:\n",
    "        The assistant's response\n",
    "    \"\"\"\n",
    "    # Step 1: Rewrite query for better search\n",
    "    optimized_query = rewrite_query(user_message)\n",
    "    print(f\"ðŸ”„ Optimized query: {optimized_query}\")\n",
    "    \n",
    "    # Step 2: Search with optimized query\n",
    "    semantic_results = search_client.search(\n",
    "        search_text=optimized_query,\n",
    "        query_type=QueryType.SEMANTIC,\n",
    "        semantic_configuration_name=\"default\",\n",
    "        top=3\n",
    "    )\n",
    "    semantic_list = list(semantic_results)\n",
    "    print(f\"ðŸ“š Found {len(semantic_list)} relevant documents\")\n",
    "    \n",
    "    # Step 3: Convert results to markdown context\n",
    "    context_md = to_markdown(semantic_list)\n",
    "    \n",
    "    # Step 4: Build messages with history\n",
    "    messages = [{\"role\": \"system\", \"content\": MULTI_TURN_SYSTEM_MESSAGE}]\n",
    "    \n",
    "    # Add conversation history\n",
    "    messages.extend(conversation_history)\n",
    "    \n",
    "    # Add current user message with context\n",
    "    current_message = f\"**New context from search:**\\n{context_md}\\n\\n**User Question:** {user_message}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": current_message})\n",
    "    \n",
    "    # Step 5: Get LLM response\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    assistant_response = response.choices[0].message.content\n",
    "    \n",
    "    # Step 6: Update conversation history (store simplified version)\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    return assistant_response\n",
    "\n",
    "print(\"âœ… Multi-turn RAG function initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47dedbc",
   "metadata": {},
   "source": [
    "## Interactive Chat Loop\n",
    "\n",
    "Run the cell below to start a multi-turn conversation. Type your questions and see how the assistant remembers context!\n",
    "\n",
    "- Type `quit` or `exit` to end the conversation\n",
    "- Type `clear` to reset conversation history\n",
    "- Try follow-up questions like \"Tell me more\" or \"What about X?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ff0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive multi-turn chat loop\n",
    "print(\"ðŸ§  Multi-Turn Philosophy RAG Chat\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Commands: 'quit'/'exit' to end, 'clear' to reset memory\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Reset conversation history for new session\n",
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nðŸ‘¤ You: \").strip()\n",
    "    \n",
    "    # Handle special commands\n",
    "    if user_input.lower() in ['quit', 'exit']:\n",
    "        print(\"\\nðŸ‘‹ Goodbye! Conversation ended.\")\n",
    "        break\n",
    "    \n",
    "    if user_input.lower() == 'clear':\n",
    "        conversation_history = []\n",
    "        print(\"ðŸ—‘ï¸ Conversation history cleared!\")\n",
    "        continue\n",
    "    \n",
    "    if not user_input:\n",
    "        print(\"Please enter a question.\")\n",
    "        continue\n",
    "    \n",
    "    # Process the message\n",
    "    print(f\"\\n{'â”€' * 50}\")\n",
    "    response = multi_turn_rag(user_input, conversation_history)\n",
    "    print(f\"\\nðŸ¤– Assistant:\\n{response}\")\n",
    "    print(f\"{'â”€' * 50}\")\n",
    "    print(f\"ðŸ“ Memory: {len(conversation_history)//2} exchanges stored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0267070e",
   "metadata": {},
   "source": [
    "## Test Memory with Follow-up Questions\n",
    "\n",
    "Try this conversation flow to test memory:\n",
    "\n",
    "1. **First question:** \"Who is Kant?\"\n",
    "2. **Follow-up:** \"What are his main ideas?\"\n",
    "3. **Another follow-up:** \"Tell me more about the categorical imperative\"\n",
    "\n",
    "The assistant should understand that \"his\" refers to Kant from the previous context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View current conversation history\n",
    "print(\"ðŸ“œ Current Conversation History:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not conversation_history:\n",
    "    print(\"No conversation history yet. Start chatting above!\")\n",
    "else:\n",
    "    for i, msg in enumerate(conversation_history):\n",
    "        role = \"ðŸ‘¤ User\" if msg[\"role\"] == \"user\" else \"ðŸ¤– Assistant\"\n",
    "        content = msg[\"content\"][:200] + \"...\" if len(msg[\"content\"]) > 200 else msg[\"content\"]\n",
    "        print(f\"\\n{role}:\\n{content}\")\n",
    "        \n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"Total exchanges: {len(conversation_history)//2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-as-a-Judge System Message for RAG Evaluation\n",
    "# Evaluates: Faithfulness, Relevance, Completeness, and Coherence\n",
    "\n",
    "LLM_JUDGE_SYSTEM_MESSAGE = \"\"\"You are an expert evaluator for RAG (Retrieval-Augmented Generation) systems.\n",
    "\n",
    "Your task: Evaluate the quality of an AI assistant's response based on the provided context and user question.\n",
    "\n",
    "## Evaluation Criteria (Score 1-5 for each):\n",
    "\n",
    "### 1. FAITHFULNESS (Is the answer grounded in the context?)\n",
    "- 5: Fully supported by context, no hallucinations\n",
    "- 4: Mostly supported, minor unsupported claims\n",
    "- 3: Partially supported, some claims lack evidence\n",
    "- 2: Mostly unsupported, significant hallucinations\n",
    "- 1: Completely fabricated or contradicts context\n",
    "\n",
    "### 2. RELEVANCE (Does it answer the user's question?)\n",
    "- 5: Directly and completely answers the question\n",
    "- 4: Answers the question with minor gaps\n",
    "- 3: Partially answers, misses key aspects\n",
    "- 2: Tangentially related, mostly off-topic\n",
    "- 1: Completely irrelevant to the question\n",
    "\n",
    "### 3. COMPLETENESS (Is the answer thorough?)\n",
    "- 5: Comprehensive, covers all relevant aspects from context\n",
    "- 4: Good coverage, minor omissions\n",
    "- 3: Covers basics, misses important details\n",
    "- 2: Superficial, significant gaps\n",
    "- 1: Extremely incomplete or empty\n",
    "\n",
    "### 4. COHERENCE (Is it well-structured and clear?)\n",
    "- 5: Excellent structure, clear and easy to follow\n",
    "- 4: Good structure, minor clarity issues\n",
    "- 3: Adequate structure, some confusion\n",
    "- 2: Poor structure, hard to follow\n",
    "- 1: Incoherent or nonsensical\n",
    "\n",
    "## Output Format (JSON):\n",
    "```json\n",
    "{\n",
    "    \"faithfulness\": {\"score\": X, \"reason\": \"brief explanation\"},\n",
    "    \"relevance\": {\"score\": X, \"reason\": \"brief explanation\"},\n",
    "    \"completeness\": {\"score\": X, \"reason\": \"brief explanation\"},\n",
    "    \"coherence\": {\"score\": X, \"reason\": \"brief explanation\"},\n",
    "    \"overall_score\": X.X,\n",
    "    \"summary\": \"One sentence overall assessment\"\n",
    "}\n",
    "```\n",
    "\n",
    "IMPORTANT: \n",
    "- Be strict but fair in your evaluation\n",
    "- Always output valid JSON only - no extra text\n",
    "- Base faithfulness ONLY on the provided context\n",
    "- The overall_score is the average of all four scores\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e5980",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
